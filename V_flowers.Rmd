---
title: "Variables reproductivas"
output:
 html_document:
   toc: true
   toc_depth: 5
   toc_float:
     collapsed: false
     smooth_scroll: true
---

# Listas

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

library(readxl)
datosff <- read_excel("database.xlsx", sheet="V repvars")

datosff$year<-as.factor(datosff$year)
datosff$site<-factor(datosff$site, levels = c("Concordia", "PN El Palmar", "Gualeguaychú"))
datosff$phenotype<-as.factor(datosff$phenotype)
datosff$id<-as.factor(datosff$id)
datosff$polen_type<-as.factor(datosff$polen_type)

datosramas <- read_excel("database.xlsx", sheet="V branches")

datosramas$year <- as.factor(datosramas$year)
datosramas$site <- factor(datosramas$site, levels = c("Concordia", "PN El Palmar", "Gualeguaychú"))
datosramas$phenotype <- as.factor(datosramas$phenotype)
datosramas$branch <- as.factor(datosramas$branch)
datosramas$NFR_NFL <- datosramas$NFR/datosramas$NFL

datosfisico <- read_excel("database.xlsx", sheet="V fisico")

datosfisico$year <- as.factor(datosfisico$year)
datosfisico$site <- factor(datosfisico$site, levels = c("Concordia", "PN El Palmar", "Gualeguaychú"))
datosfisico$phenotype <- as.factor(datosfisico$phenotype)
datosfisico$id <- as.factor(datosfisico$id)
datosfisico$mad <- as.factor(datosfisico$mad)
# Primero, filtramos las filas donde PSS no es NA ni cero
datosfisico_filtrado <- datosfisico[!is.na(datosfisico$PSS) & datosfisico$PSS != 0, ]
datosfisico_filtrado$PSS_PSF <- datosfisico_filtrado$PSS / datosfisico_filtrado$PSF
datosfisico <- datosfisico_filtrado

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# Media y CV%
# Primero, cargamos las librerías necesarias si no están ya cargadas
library(dplyr)
datosff_mean <- datosff[,-c(1:4,6)]
datosramas_mean <- datosramas[,-c(1:4)]
datosfisico_mean <- datosfisico[,-c(1:4,9)]

# Calculamos la media y el CV% de cada variable numérica
resultados <- datosff_mean %>%
  dplyr::summarise(dplyr::across(dplyr::where(is.numeric), list(
    mean = ~mean(., na.rm = TRUE),
    CV = ~sd(., na.rm = TRUE) / mean(., na.rm = TRUE) * 100
  )))

# Muestra el resultado
print(resultados)
```

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# Tablas y gráficos resumen

# Crea un vector con las variables respuesta
# Crear un vector con los nombres específicos de las variables
variables_respuesta <- c("PSFL", "DMAP", "DMIP", "NO", "NS", "NS_NO")

# Inicializa listas para almacenar las tablas y gráficos
tablas_list <- list()
graficos_list <- list()

library(dplyr)
library(ggplot2)
# Define un vector con las etiquetas de las unidades para cada variable
unidades <- c("PSFL" = "Peso (mg)", 
              "DMAP" = "DMAP", 
              "DMIP" = "DMIP", 
              "NO" = "Número de óvulos", 
              "NS" = "Número de semillas", 
              "NS_NO" = "NS/NO")

# Define los colores a utilizar en los gráficos
colores <- c("coral1", "burlywood2", "palegreen4")

# Crea un bucle para recorrer cada variable respuesta
for (variable in variables_respuesta) {

  # Crea la tabla resumen
  assign(paste0("Tabla_", variable), datosff %>%
    dplyr::filter(!is.na(get(variable))) %>% # Filtra los datos sin NA para la variable actual
    dplyr::group_by(year, site, phenotype) %>%
    dplyr::summarise(n = n(),
              Mean = mean(get(variable)),
              sd = sd(get(variable))
              )
  )
  
  # Agrega la tabla a la lista
  tablas_list[[variable]] <- get(paste0("Tabla_", variable))
  
  # Obtén la unidad correspondiente a la variable actual
  y_label <- unidades[variable]

  # Crea el gráfico
  ggplotff <- ggplot(data = tablas_list[[variable]], aes(x = phenotype, y = Mean, fill = site)) +
    stat_summary(fun = "mean", size = 1, geom = "bar") +
    geom_errorbar(aes(ymin = Mean - sd, ymax = Mean + sd),
                  width = .2, position = position_dodge(.9)) +
    facet_grid(year ~ .) +
    labs(x = "", y = y_label) + # Utiliza la etiqueta dinámica para el eje y
    theme_classic() +
    theme(plot.title = element_text(hjust = 0.5, size = rel(1.5))) +
    scale_fill_manual(values = colores) +
    theme(legend.title = element_blank()) +
    theme(legend.position = 'bottom') +
    theme(panel.grid.major.y = element_line(color = 'black')) +
    theme(panel.grid.major.x = element_blank()) +
    theme(text = element_text(size = 30, color = 'black')) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, color = 'black')) +
    theme(axis.text.y = element_text(angle = 0, hjust = 1, color = 'black'))
  
  # Asigna el gráfico a un objeto dinámico
  assign(paste0("flores_gg_", variable), ggplotff)
  
  # Agrega el gráfico a la lista
  graficos_list[[variable]] <- get(paste0("flores_gg_", variable))
}

```

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# Tablas resumen

# Crea un vector con las variables respuesta
# Crear un vector con los nombres específicos de las variables
variables_respuesta <- c("NFR_NFL", "NFL_cm")

library(dplyr)
library(ggplot2)

# Define un vector con las etiquetas de las unidades para cada variable
unidades <- c("NFR_NFL" = "NFR/NFL", 
              "NFL_cm" = "NFL/cm")

# Define los colores a utilizar en los gráficos
colores <- c("coral1", "burlywood2", "palegreen4")

# Crea un bucle para recorrer cada variable respuesta
for (variable in variables_respuesta) {

  # Crea la tabla resumen
  assign(paste0("Tabla_", variable), datosramas %>%
    dplyr::filter(!is.na(get(variable))) %>% # Filtra los datos sin NA para la variable actual
    dplyr::group_by(year, site, phenotype) %>%
    dplyr::summarise(n = n(),
              Mean = mean(get(variable)),
              sd = sd(get(variable))
              )
  )
  
  # Agrega la tabla a la lista
  tablas_list[[variable]] <- get(paste0("Tabla_", variable))
  
  # Obtén la unidad correspondiente a la variable actual
  y_label <- unidades[variable]

  # Crea el gráfico
  ggplotff <- ggplot(data = tablas_list[[variable]], aes(x = phenotype, y = Mean, fill = site)) +
    stat_summary(fun = "mean", size = 1, geom = "bar") +
    geom_errorbar(aes(ymin = Mean - sd, ymax = Mean + sd),
                  width = .2, position = position_dodge(.9)) +
    facet_grid(year ~ .) +
    labs(x = "", y = y_label) + # Utiliza la etiqueta dinámica para el eje y
    theme_classic() +
    theme(plot.title = element_text(hjust = 0.5, size = rel(1.5))) +
    scale_fill_manual(values = colores) +
    theme(legend.title = element_blank()) +
    theme(legend.position = 'bottom') +
    theme(panel.grid.major.y = element_line(color = 'black')) +
    theme(panel.grid.major.x = element_blank()) +
    theme(text = element_text(size = 30, color = 'black')) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, color = 'black')) +
    theme(axis.text.y = element_text(angle = 0, hjust = 1, color = 'black'))
  
  # Asigna el gráfico a un objeto dinámico
  assign(paste0("flores_gg_", variable), ggplotff)
  
  # Agrega el gráfico a la lista
  graficos_list[[variable]] <- get(paste0("flores_gg_", variable))
}

```

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# Tablas resumen

# Crea un vector con las variables respuesta
# Crear un vector con los nombres específicos de las variables
variables_respuesta <- c("PSS", "PSF", "PSS_PSF")

library(dplyr)
library(ggplot2)

# Define un vector con las etiquetas de las unidades para cada variable
unidades <- c("PSS" = "PSS (g)", 
              "PSF" = "PSF (g)",
              "PSS_PSF" = "PSS/PSF")

# Define los colores a utilizar en los gráficos
colores <- c("coral1", "burlywood2", "palegreen4")

# Crea un bucle para recorrer cada variable respuesta
for (variable in variables_respuesta) {

  # Crea la tabla resumen
  assign(paste0("Tabla_", variable), datosfisico %>%
    dplyr::filter(!is.na(get(variable))) %>% # Filtra los datos sin NA para la variable actual
    dplyr::group_by(year, site, phenotype) %>%
    dplyr::summarise(n = n(),
              Mean = mean(get(variable)),
              sd = sd(get(variable))
              )
  )
  
  # Agrega la tabla a la lista
  tablas_list[[variable]] <- get(paste0("Tabla_", variable))
  
  # Obtén la unidad correspondiente a la variable actual
  y_label <- unidades[variable]

  # Crea el gráfico
  ggplotff <- ggplot(data = tablas_list[[variable]], aes(x = phenotype, y = Mean, fill = site)) +
    stat_summary(fun = "mean", size = 1, geom = "bar") +
    geom_errorbar(aes(ymin = Mean - sd, ymax = Mean + sd),
                  width = .2, position = position_dodge(.9)) +
    facet_grid(year ~ .) +
    labs(x = "", y = y_label) + # Utiliza la etiqueta dinámica para el eje y
    theme_classic() +
    theme(plot.title = element_text(hjust = 0.5, size = rel(1.5))) +
    scale_fill_manual(values = colores) +
    theme(legend.title = element_blank()) +
    theme(legend.position = 'bottom') +
    theme(panel.grid.major.y = element_line(color = 'black')) +
    theme(panel.grid.major.x = element_blank()) +
    theme(text = element_text(size = 30, color = 'black')) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, color = 'black')) +
    theme(axis.text.y = element_text(angle = 0, hjust = 1, color = 'black'))
  
  # Asigna el gráfico a un objeto dinámico
  assign(paste0("flores_gg_", variable), ggplotff)
  
  # Agrega el gráfico a la lista
  graficos_list[[variable]] <- get(paste0("flores_gg_", variable))
}

```

# Estimación de componentes de varianza

## Peso seco de botones florales en fases C/D (PSFL)
```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

graficos_list$PSFL

```
Resumen
```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

library(rptR)
library(pbapply)

rpt_PSFL<-rptGaussian(PSFL ~ (1|year) + (1|site) + (1|phenotype),
                    grname=c("year", "site", "phenotype", "Residual"),
                    data=datosff, nboot=3, npermut=3)
summary(rpt_PSFL)

```

## Diámetro máximo de polen (DMAP)
```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

graficos_list$DMAP

```
Resumen
```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# library(rptR)
# library(pbapply)
# 
# rpt_DMAP<-rptGaussian(DMAP ~ (1|year) + (1|site) + (1|phenotype),
#                     grname=c("year", "site", "phenotype", "Residual"),
#                     data=datosff, nboot=3, npermut=3)
# summary(rpt_DMAP)

# library(rptR)
# library(pbapply)
# datosff$DMAP <- as.integer(datosff$DMAP)
# rpt_DMAP<-rptPoisson(DMAP ~ (1|year) + (1|site) + (1|phenotype),
#                     grname=c("year", "site", "phenotype", "Residual", "Overdispersion"),
#                     data=datosff,
#                     link = c("log"), nboot=3, npermut=3)
# summary(rpt_DMAP)

# library(lme4)
# 
# Gaussian_DMAP <- lmer(DMAP ~ (1|year) + (1|site/phenotype), 
#                data = datosff,
#               )
# 
# residuals_Gaussian <- residuals(Gaussian_DMAP)
# library(car)
# # Gráfico Q-Q para chequear normalidad
# qqPlot(residuals_Gaussian, main = "Q-Q Plot de Residuos")
# 
# # Prueba de Shapiro-Wilk para normalidad (aunque menos recomendada para grandes muestras)
# shapiro.test(residuals_Gaussian)
# 
# # Gráfico de residuos vs valores ajustados para chequear homogeneidad de varianzas
# plot(fitted(Gaussian_DMAP), residuals_Gaussian, 
#      xlab = "Valores Ajustados", ylab = "Residuos", 
#      main = "Residuos vs Valores Ajustados")
# abline(h = 0, col = "red")

# library(lme4)
datosff$DMAP <- as.integer(datosff$DMAP)
# Poisson_DMAP <- glmer(DMAP ~ (1|year) + (1|site/phenotype), 
#                data = datosff,
#                family = poisson(link = "log"))
# 
# #Residuos 
# names(Poisson_DMAP)
# rd<-resid(Poisson_DMAP) # residuos devianza
# rp<-resid(Poisson_DMAP, type="pearson") #residuos de Pearson
# ajust<-fitted(Poisson_DMAP) #predichos en escala de Y
# 
# #Grafico de residuos vs predichos
# plot(ajust, rp,xlab="predichos", ylab= "Residuos de pearson", main="Gráfico de RP vs PRED")
# abline(0,0)
# plot(cooks.distance(Poisson_DMAP), type="h", xlab="Id", ylab= "Distancia de Cook", main="Gráfico de Distancia de Cook")
# 
# # Dispersión
# residuals_pearson <- residuals(Poisson_DMAP, type = "pearson")
# df_residual <- df.residual(Poisson_DMAP)
# 
# dispersion <- sum(residuals_pearson^2) / df_residual # HAY SUBDISPERSIÓN

library(glmmTMB)
# Ajuste del modelo completo con distribución compois
CMP_DMAP <- glmmTMB(DMAP ~ (1|year) + (1|site/phenotype), 
                    data = datosff, 
                    family = compois())

# # Dispersión
# residuals_pearson <- residuals(CMP_DMAP, type = "pearson")
# df_residual <- df.residual(CMP_DMAP)

# dispersion <- sum(residuals_pearson^2) / df_residual

# # Encuentra el índice del valor absoluto máximo de los residuos
# outlier_index <- which.max(abs(residuals_CMP))
# 
# # Muestra los detalles del outlier
# cat("Índice del outlier:", outlier_index, "\n")
# cat("Valor ajustado del outlier:", ajust[outlier_index], "\n")
# cat("Residuo de deviance del outlier:", rp[outlier_index], "\n")
# cat("Datos originales del outlier:\n")
# print(datosff[outlier_index, ])


```

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)


# Extraer varianzas
var_comps <- VarCorr(CMP_DMAP)
var_year <- tryCatch(as.numeric(var_comps$cond$year[1]), error = function(e) NA)
var_site_phenotype <- tryCatch(as.numeric(var_comps$cond$`phenotype:site`[1]), error = function(e) NA)
var_site <- tryCatch(as.numeric(var_comps$cond$site[1]), error = function(e) NA)
var_residual <- tryCatch(sigma(CMP_DMAP)^2, error = function(e) NA)

total_var <- sum(var_year, var_site_phenotype, var_site, var_residual, na.rm = TRUE)
r_year <- (var_year / total_var) * 100
r_site_phenotype <- (var_site_phenotype / total_var) * 100
r_site <- (var_site / total_var) * 100
r_residual <- (var_residual / total_var) * 100

# Ajustar modelos reducidos
CMP_DMAP_year <- tryCatch({
  glmmTMB(DMAP ~ (1|site/phenotype), 
          data = datosff, 
          family = compois())
}, error = function(e) {
  warning("Error ajustando el modelo reducido para year: ", e$message)
  return(NULL)
})

CMP_DMAP_site <- tryCatch({
  glmmTMB(DMAP ~ (1|year) + (1|phenotype), 
          data = datosff, 
          family = compois())
}, error = function(e) {
  warning("Error ajustando el modelo reducido para site: ", e$message)
  return(NULL)
})

CMP_DMAP_phenotype <- tryCatch({
  glmmTMB(DMAP ~ (1|year) + (1|site), 
          data = datosff, 
          family = compois())
}, error = function(e) {
  warning("Error ajustando el modelo reducido para phenotype: ", e$message)
  return(NULL)
})

# Función para calcular p-valor mediante LRT, adaptada según rptR
calculate_LRT_p_value <- function(full_model, reduced_model) {
  if (is.null(full_model) || is.null(reduced_model)) return(NA)
  
  logLik_full <- logLik(full_model)
  logLik_reduced <- logLik(reduced_model)
  
  # Diferencia de log-verosimilitud multiplicada por 2
  LR_D <- -2 * (logLik_reduced - logLik_full)
  
  # Grados de libertad (esto puede variar dependiendo del número de términos aleatorios removidos)
  df <- 1  # Asumiendo que quitamos un solo término aleatorio
  
  # P-valor usando chi-cuadrado
  p_value <- pchisq(LR_D, df, lower.tail = FALSE)
  
  # Corrección para prueba unilateral cuando solo se elimina un parámetro
  if (df == 1 && LR_D > 0) {
    p_value <- p_value / 2
  }
  
  return(p_value)
}

# Calcular p-valores
p_year <- calculate_LRT_p_value(CMP_DMAP, CMP_DMAP_year)
p_site <- calculate_LRT_p_value(CMP_DMAP, CMP_DMAP_site)
p_phenotype <- calculate_LRT_p_value(CMP_DMAP, CMP_DMAP_phenotype)

# Crear un data frame con los resultados
results_DMAP <- data.frame(
  Component = c("Year", "Site:Phenotype", "Site", "Residual"),
  r = c(r_year, r_site_phenotype, r_site, r_residual),
  p_value = c(p_year, p_phenotype, p_site, NA)  # No hay p-value para el residual en este contexto
)

# Mostrar la tabla
print(results_DMAP)

```

## Diámetro mínimo de polen (DMIP)
```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

graficos_list$DMAP

```
Resumen
```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# library(rptR)
# library(pbapply)
# 
# rpt_DMIP<-rptGaussian(DMIP ~ (1|year) + (1|site) + (1|phenotype),
#                     grname=c("year", "site", "phenotype", "Residual"),
#                     data=datosff, nboot=3, npermut=3)
# summary(rpt_DMIP)

# library(rptR)
# library(pbapply)
datosff$DMIP <- as.integer(datosff$DMIP)
# rpt_DMIP<-rptPoisson(DMIP ~ (1|year) + (1|site) + (1|phenotype),
#                     grname=c("year", "site", "phenotype", "Residual", "Overdispersion"),
#                     data=datosff,
#                     link = c("log"), nboot=3, npermut=3)
# summary(rpt_DMIP)

# library(lme4)
# 
# Gaussian_DMIP <- lmer(DMIP ~ (1|year) + (1|site/phenotype), 
#                data = datosff,
#               )
# 
# residuals_Gaussian <- residuals(Gaussian_DMIP)
# library(car)
# # Gráfico Q-Q para chequear normalidad
# qqPlot(residuals_Gaussian, main = "Q-Q Plot de Residuos")
# 
# # Prueba de Shapiro-Wilk para normalidad (aunque menos recomendada para grandes muestras)
# shapiro.test(residuals_Gaussian)
# 
# # Gráfico de residuos vs valores ajustados para chequear homogeneidad de varianzas
# plot(fitted(Gaussian_DMIP), residuals_Gaussian, 
#      xlab = "Valores Ajustados", ylab = "Residuos", 
#      main = "Residuos vs Valores Ajustados")
# abline(h = 0, col = "red")

# library(lme4)
# datosff$DMIP <- as.integer(datosff$DMIP)
# Poisson_DMIP <- glmer(DMIP ~ (1|year) + (1|site/phenotype), 
#                data = datosff,
#                family = poisson(link = "log"))
# 
# #Residuos 
# names(Poisson_DMIP)
# rd<-resid(Poisson_DMIP) # residuos devianza
# rp<-resid(Poisson_DMIP, type="pearson") #residuos de Pearson
# ajust<-fitted(Poisson_DMIP) #predichos en escala de Y
# 
# #Grafico de residuos vs predichos
# plot(ajust, rp,xlab="predichos", ylab= "Residuos de pearson", main="Gráfico de RP vs PRED")
# abline(0,0)
# plot(cooks.distance(Poisson_DMIP), type="h", xlab="Id", ylab= "Distancia de Cook", main="Gráfico de Distancia de Cook")
# 
# # Dispersión
# residuals_pearson <- residuals(Poisson_DMIP, type = "pearson")
# df_residual <- df.residual(Poisson_DMIP)
# 
# dispersion <- sum(residuals_pearson^2) / df_residual # HAY SUBDISPERSIÓN

library(glmmTMB)
# Ajuste del modelo completo con distribución compois
CMP_DMIP <- glmmTMB(DMIP ~ (1|year) + (1|site/phenotype), 
                    data = datosff, 
                    family = compois())

# Dispersión
residuals_pearson <- residuals(CMP_DMIP, type = "pearson")
df_residual <- df.residual(CMP_DMIP)

dispersion <- sum(residuals_pearson^2) / df_residual

# # Encuentra el índice del valor absoluto máximo de los residuos
# outlier_index <- which.max(abs(residuals_CMP))
# 
# # Muestra los detalles del outlier
# cat("Índice del outlier:", outlier_index, "\n")
# cat("Valor ajustado del outlier:", ajust[outlier_index], "\n")
# cat("Residuo de deviance del outlier:", rp[outlier_index], "\n")
# cat("Datos originales del outlier:\n")
# print(datosff[outlier_index, ])



# Extraer varianzas
var_comps <- VarCorr(CMP_DMIP)
var_year <- tryCatch(as.numeric(var_comps$cond$year[1]), error = function(e) NA)
var_site_phenotype <- tryCatch(as.numeric(var_comps$cond$`phenotype:site`[1]), error = function(e) NA)
var_site <- tryCatch(as.numeric(var_comps$cond$site[1]), error = function(e) NA)
var_residual <- tryCatch(sigma(CMP_DMIP)^2, error = function(e) NA)

total_var <- sum(var_year, var_site_phenotype, var_site, var_residual, na.rm = TRUE)
r_year <- (var_year / total_var) * 100
r_site_phenotype <- (var_site_phenotype / total_var) * 100
r_site <- (var_site / total_var) * 100
r_residual <- (var_residual / total_var) * 100

# Ajustar modelos reducidos
CMP_DMIP_year <- tryCatch({
  glmmTMB(DMIP ~ (1|site/phenotype), 
          data = datosff, 
          family = compois())
}, error = function(e) {
  warning("Error ajustando el modelo reducido para year: ", e$message)
  return(NULL)
})

CMP_DMIP_site <- tryCatch({
  glmmTMB(DMIP ~ (1|year) + (1|phenotype), 
          data = datosff, 
          family = compois())
}, error = function(e) {
  warning("Error ajustando el modelo reducido para site: ", e$message)
  return(NULL)
})

CMP_DMIP_phenotype <- tryCatch({
  glmmTMB(DMIP ~ (1|year) + (1|site), 
          data = datosff, 
          family = compois())
}, error = function(e) {
  warning("Error ajustando el modelo reducido para phenotype: ", e$message)
  return(NULL)
})

# Función para calcular p-valor mediante LRT, adaptada según rptR
calculate_LRT_p_value <- function(full_model, reduced_model) {
  if (is.null(full_model) || is.null(reduced_model)) return(NA)
  
  logLik_full <- logLik(full_model)
  logLik_reduced <- logLik(reduced_model)
  
  # Diferencia de log-verosimilitud multiplicada por 2
  LR_D <- -2 * (logLik_reduced - logLik_full)
  
  # Grados de libertad (esto puede variar dependiendo del número de términos aleatorios removidos)
  df <- 1  # Asumiendo que quitamos un solo término aleatorio
  
  # P-valor usando chi-cuadrado
  p_value <- pchisq(LR_D, df, lower.tail = FALSE)
  
  # Corrección para prueba unilateral cuando solo se elimina un parámetro
  if (df == 1 && LR_D > 0) {
    p_value <- p_value / 2
  }
  
  return(p_value)
}

# Calcular p-valores
p_year <- calculate_LRT_p_value(CMP_DMIP, CMP_DMIP_year)
p_site <- calculate_LRT_p_value(CMP_DMIP, CMP_DMIP_site)
p_phenotype <- calculate_LRT_p_value(CMP_DMIP, CMP_DMIP_phenotype)

# Crear un data frame con los resultados
results_DMIP <- data.frame(
  Component = c("Year", "Site:Phenotype", "Site", "Residual"),
  r = c(r_year, r_site_phenotype, r_site, r_residual),
  p_value = c(p_year, p_phenotype, p_site, NA)  # No hay p-value para el residual en este contexto
)

# Mostrar la tabla
print(results_DMIP)

```

## Número de óvulos por flor
```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

graficos_list$NO

```
Resumen
```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

library(rptR)
library(pbapply)

rpt_NO<-rptPoisson(NO ~ (1|year) + (1|site) + (1|phenotype),
                    grname=c("year", "site", "phenotype", "Residual", "Overdispersion"),
                    data=datosff,
                    link = c("sqrt"), nboot=3, npermut=3)
summary(rpt_NO)

```

## Número de semillas por fruto
```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

graficos_list$NS

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

library(rptR)
library(pbapply)

rpt_NS<-rptPoisson(NS ~ (1|year) + (1|site) + (1|phenotype),
                    grname=c("year", "site", "phenotype", "Residual", "Overdispersion"),
                    data=datosff,
                    link = c("sqrt"), nboot=3, npermut=3)
summary(rpt_NS)

```

## NS/NO

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

graficos_list$NS_NO

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

datosff_clean_NS_NO <- datosff[!is.na(datosff$NS_NO), ]
# datosff_clean_NS_NO$NS_NO_log <- log(datosff_clean$NS_NO+1)
# 
# library(rptR)
# library(pbapply)
# 
# rpt_NSNO_<-rptGaussian((NS_NO) ~ (1|year) + (1|site) + (1|phenotype),
#                     grname=c("year", "site", "phenotype", "Residual"),
#                     data=datosff_clean_NS_NO, nboot=3, npermut=3)
# summary(rpt_NSNO_log)

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

datosff_clean_NS_NO <- datosff[!is.na(datosff$NS_NO), ]
# Ajustar NS/NO para eliminar ceros y unos
datosff_clean_NS_NO$adjusted_NS_NO <- pmin(pmax(datosff_clean_NS_NO$NS_NO, 0.001), 0.999)

library(glmmTMB)

# Ajustar el modelo completo con distribución Beta
beta_adjusted_NS_NO <- tryCatch({
  glmmTMB(adjusted_NS_NO ~ 1 + (1|year) + (1|site) + (1|phenotype), 
          data = datosff_clean_NS_NO, 
          family = beta_family(link = "logit"))
}, error = function(e) {
  warning("Error ajustando el modelo completo: ", e$message)
  return(NULL)
})

if (is.null(beta_adjusted_NS_NO)) {
  stop("No se pudo ajustar el modelo completo para adjusted_NS_NO.")
}

# Extraer varianzas
var_comps <- VarCorr(beta_adjusted_NS_NO)
var_year <- tryCatch(as.numeric(var_comps$cond$year[1]), error = function(e) NA)
var_site <- tryCatch(as.numeric(var_comps$cond$site[1]), error = function(e) NA)
var_phenotype <- tryCatch(as.numeric(var_comps$cond$phenotype[1]), error = function(e) NA)
var_residual <- tryCatch(sigma(beta_adjusted_NS_NO)^2, error = function(e) NA)

total_var <- sum(var_year, var_site, var_phenotype, var_residual, na.rm = TRUE)
r_year <- (var_year / total_var) * 100
r_site <- (var_site / total_var) * 100
r_phenotype <- (var_phenotype / total_var) * 100
r_residual <- (var_residual / total_var) * 100

# Ajustar modelos reducidos
beta_adjusted_NS_NO_year <- tryCatch({
  glmmTMB(adjusted_NS_NO ~ 1 + (1|site) + (1|phenotype), 
          data = datosff_clean_NS_NO, 
          family = beta_family(link = "logit"))
}, error = function(e) {
  warning("Error ajustando el modelo reducido para year: ", e$message)
  return(NULL)
})

beta_adjusted_NS_NO_site <- tryCatch({
  glmmTMB(adjusted_NS_NO ~ 1 + (1|year) + (1|phenotype), 
          data = datosff_clean_NS_NO, 
          family = beta_family(link = "logit"))
}, error = function(e) {
  warning("Error ajustando el modelo reducido para site: ", e$message)
  return(NULL)
})

beta_adjusted_NS_NO_phenotype <- tryCatch({
  glmmTMB(adjusted_NS_NO ~ 1 + (1|year) + (1|site), 
          data = datosff_clean_NS_NO, 
          family = beta_family(link = "logit"))
}, error = function(e) {
  warning("Error ajustando el modelo reducido para phenotype: ", e$message)
  return(NULL)
})

# Función para calcular p-valor mediante LRT, ajustada según rptR
calculate_LRT_p_value <- function(full_model, reduced_model) {
  if (is.null(full_model) || is.null(reduced_model)) return(NA)
  
  logLik_full <- logLik(full_model)
  logLik_reduced <- logLik(reduced_model)
  
  # Diferencia de log-verosimilitud multiplicada por 2
  LR_D <- -2 * (logLik_reduced - logLik_full)
  
  # Grados de libertad (esto puede variar dependiendo del número de términos aleatorios removidos)
  df <- 1  # Asumiendo que quitamos un solo término aleatorio
  
  # P-valor usando chi-cuadrado
  p_value <- pchisq(LR_D, df, lower.tail = FALSE)
  
  # Corrección para prueba unilateral cuando solo se elimina un parámetro
  if (df == 1 && LR_D > 0) {
    p_value <- p_value / 2
  }
  
  return(p_value)
}

# Calcular p-valores
p_year <- calculate_LRT_p_value(beta_adjusted_NS_NO, beta_adjusted_NS_NO_year)
p_site <- calculate_LRT_p_value(beta_adjusted_NS_NO, beta_adjusted_NS_NO_site)
p_phenotype <- calculate_LRT_p_value(beta_adjusted_NS_NO, beta_adjusted_NS_NO_phenotype)

# Crear un data frame con los resultados
results_adjusted_NS_NO <- data.frame(
  Component = c("Year", "Site", "Phenotype", "Residual"),
  r = c(r_year, r_site, r_phenotype, r_residual),
  p_value = c(p_year, p_site, p_phenotype, NA)  # No hay p-value para el residual en este contexto
)

# Mostrar la tabla
print(results_adjusted_NS_NO)

```

## NFL/cm

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

graficos_list$NFL_cm

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

datosramas$NFL_cm <- datosramas$NFL_cm+0.001
datosramas_clean <- datosramas[!is.na(datosramas$NFL_cm), ]
datosramas_clean$NFL_cm_log <- log(datosramas_clean$NFL_cm+1)

library(rptR)
library(pbapply)

rpt_NFL_cm_log<-rptGaussian(NFL_cm_log ~ (1|year) + (1|site) + (1|phenotype),
                    grname=c("year", "site", "phenotype", "Residual"),
                    data=datosramas_clean, nboot=3, npermut=3)
summary(rpt_NFL_cm_log)

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE, out.width='75%')

datosramas$NFL_cm <- datosramas$NFL_cm + 0.001

library(lme4)

# Ajustar el modelo completo con distribución Gamma
gamma_NFL_cm <- tryCatch({
  glmer(NFL_cm ~ 1 + (1|year) + (1|site) + (1|phenotype), 
        data = datosramas, 
        family = Gamma(link = "log"),
        control = glmerControl(optimizer = "bobyqa"))
}, error = function(e) {
  warning("Error ajustando el modelo completo: ", e$message)
  return(NULL)
})

if (is.null(gamma_NFL_cm)) {
  stop("No se pudo ajustar el modelo completo para NFL_cm.")
}

# Extraer varianzas
var_comps <- VarCorr(gamma_NFL_cm)
var_year <- tryCatch(as.numeric(var_comps$year[1]), error = function(e) NA)
var_site <- tryCatch(as.numeric(var_comps$site[1]), error = function(e) NA)
var_phenotype <- tryCatch(as.numeric(var_comps$phenotype[1]), error = function(e) NA)
var_residual <- tryCatch(attr(var_comps, "sc")^2, error = function(e) NA)

total_var <- sum(var_year, var_site, var_phenotype, var_residual, na.rm = TRUE)
r_year <- (var_year / total_var) * 100
r_site <- (var_site / total_var) * 100
r_phenotype <- (var_phenotype / total_var) * 100
r_residual <- (var_residual / total_var) * 100

# Ajustar modelos reducidos
gamma_NFL_cm_year <- tryCatch({
  glmer(NFL_cm ~ 1 + (1|site) + (1|phenotype), 
        data = datosramas, 
        family = Gamma(link = "log"),
        control = glmerControl(optimizer = "bobyqa"))
}, error = function(e) {
  warning("Error ajustando el modelo reducido para year: ", e$message)
  return(NULL)
})

gamma_NFL_cm_site <- tryCatch({
  glmer(NFL_cm ~ 1 + (1|year) + (1|phenotype), 
        data = datosramas, 
        family = Gamma(link = "log"),
        control = glmerControl(optimizer = "bobyqa"))
}, error = function(e) {
  warning("Error ajustando el modelo reducido para site: ", e$message)
  return(NULL)
})

gamma_NFL_cm_phenotype <- tryCatch({
  glmer(NFL_cm ~ 1 + (1|year) + (1|site), 
        data = datosramas, 
        family = Gamma(link = "log"),
        control = glmerControl(optimizer = "bobyqa"))
}, error = function(e) {
  warning("Error ajustando el modelo reducido para phenotype: ", e$message)
  return(NULL)
})

# Función para calcular p-valor mediante LRT, ajustada según rptR
calculate_LRT_p_value <- function(full_model, reduced_model) {
  if (is.null(full_model) || is.null(reduced_model)) return(NA)
  
  logLik_full <- logLik(full_model)
  logLik_reduced <- logLik(reduced_model)
  
  # Diferencia de log-verosimilitud multiplicada por 2
  LR_D <- -2 * (logLik_reduced - logLik_full)
  
  # Grados de libertad (esto puede variar dependiendo del número de términos aleatorios removidos)
  df <- 1  # Asumiendo que quitamos un solo término aleatorio
  
  # P-valor usando chi-cuadrado
  p_value <- pchisq(LR_D, df, lower.tail = FALSE)
  
  # Corrección para prueba unilateral cuando solo se elimina un parámetro
  if (df == 1 && LR_D > 0) {
    p_value <- p_value / 2
  }
  
  return(p_value)
}

# Calcular p-valores
p_year <- calculate_LRT_p_value(gamma_NFL_cm, gamma_NFL_cm_year)
p_site <- calculate_LRT_p_value(gamma_NFL_cm, gamma_NFL_cm_site)
p_phenotype <- calculate_LRT_p_value(gamma_NFL_cm, gamma_NFL_cm_phenotype)

# Crear un data frame con los resultados
results_NFL_cm <- data.frame(
  Component = c("Year", "Site", "Phenotype", "Residual"),
  r = c(r_year, r_site, r_phenotype, r_residual),
  p_value = c(p_year, p_site, p_phenotype, NA)  # No hay p-value para el residual en este contexto
)

# Mostrar la tabla
print(results_NFL_cm)

```

## NFR/NFL

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

graficos_list$NFR_NFL

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE, out.width='75%')

datosramas_clean <- datosramas[!is.na(datosramas$NFR_NFL), ]
# Añadir una pequeña constante para evitar ceros exactos
epsilon <- 1e-6  # Puedes ajustar este valor según sea necesario

datosramas_clean$NFR_NFL_transformed <- ifelse(datosramas_clean$NFR_NFL == 1, 1 - epsilon,
                                                ifelse(datosramas_clean$NFR_NFL == 0, epsilon,
                                                       datosramas_clean$NFR_NFL))
       
# library(rptR)
# library(pbapply)
# rpt_NFR_NFL_log<-rptGaussian((NFR_NFL_log) ~ (1|year) + (1|site) + (1|phenotype),
#                     grname=c("year", "site", "phenotype", "Residual"),
#                     data=datosramas_clean, nboot=3, npermut=3)
# summary(rpt_NFR_NFL_log)

library(glmmTMB)

# Modelo que incluye una parte para los ceros y otra para la distribución beta
model_NFR_NFL <- glmmTMB(NFR_NFL_transformed ~ 1 + (1|year) + (1|site) + (1|phenotype),
                     data = datosramas_clean,
                     family = beta_family(link = "logit"))

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE, out.width='75%')

library(glmmTMB)

# Ajustar el modelo completo con distribución Beta
beta_NFR_NFL <- tryCatch({
  glmmTMB(NFR_NFL_transformed ~ 1 + (1|year) + (1|site) + (1|phenotype), 
          data = datosramas_clean, 
          family = beta_family(link = "logit"),
          control = glmmTMBControl(optCtrl = list(iter.max=1e3, eval.max=1e3)))
}, error = function(e) {
  warning("Error ajustando el modelo completo con control de optimización: ", e$message)
  return(NULL)
})

if (is.null(beta_NFR_NFL)) {
  stop("No se pudo ajustar el modelo completo para NFR_NFL.")
}

# Extraer varianzas
var_comps <- VarCorr(beta_NFR_NFL)
var_year <- tryCatch(as.numeric(var_comps$cond$year[1]), error = function(e) NA)
var_site <- tryCatch(as.numeric(var_comps$cond$site[1]), error = function(e) NA)
var_phenotype <- tryCatch(as.numeric(var_comps$cond$phenotype[1]), error = function(e) NA)
var_residual <- tryCatch(sigma(beta_NFR_NFL)^2, error = function(e) NA)

total_var <- sum(var_year, var_site, var_phenotype, var_residual, na.rm = TRUE)
r_year <- (var_year / total_var) * 100
r_site <- (var_site / total_var) * 100
r_phenotype <- (var_phenotype / total_var) * 100
r_residual <- (var_residual / total_var) * 100

# Ajustar modelos reducidos
beta_NFR_NFL_year <- tryCatch({
  glmmTMB(NFR_NFL_transformed ~ 1 + (1|site) + (1|phenotype), 
          data = datosramas_clean, 
          family = beta_family(link = "logit"))
}, error = function(e) {
  warning("Error ajustando el modelo reducido para year: ", e$message)
  return(NULL)
})

beta_NFR_NFL_site <- tryCatch({
  glmmTMB(NFR_NFL_transformed ~ 1 + (1|year) + (1|phenotype), 
          data = datosramas_clean, 
          family = beta_family(link = "logit"))
}, error = function(e) {
  warning("Error ajustando el modelo reducido para site: ", e$message)
  return(NULL)
})

beta_NFR_NFL_phenotype <- tryCatch({
  glmmTMB(NFR_NFL_transformed ~ 1 + (1|year) + (1|site), 
          data = datosramas_clean, 
          family = beta_family(link = "logit"))
}, error = function(e) {
  warning("Error ajustando el modelo reducido para phenotype: ", e$message)
  return(NULL)
})

# Función para calcular p-valor mediante LRT, ajustada según rptR
calculate_LRT_p_value <- function(full_model, reduced_model) {
  if (is.null(full_model) || is.null(reduced_model)) return(NA)
  
  logLik_full <- logLik(full_model)
  logLik_reduced <- logLik(reduced_model)
  
  # Verificar que las log-verosimilitudes no sean NA
  if (is.na(logLik_full) || is.na(logLik_reduced)) {
    warning("Log-likelihood is NA for one or both models.")
    return(NA)
  }
  
  # Diferencia de log-verosimilitud multiplicada por 2
  LR_D <- -2 * (logLik_reduced - logLik_full)
  
  # Grados de libertad (esto puede variar dependiendo del número de términos aleatorios removidos)
  df <- 1  # Asumiendo que quitamos un solo término aleatorio
  
  # Verificación adicional para evitar NA en la condición
  if (is.na(LR_D) || is.na(df)) {
    warning("LR_D or df is NA, cannot compute p-value.")
    return(NA)
  }
  
  # P-valor usando chi-cuadrado
  p_value <- pchisq(LR_D, df, lower.tail = FALSE)
  
  # Corrección para prueba unilateral cuando solo se elimina un parámetro
  if (df == 1 && LR_D > 0) {
    p_value <- p_value / 2
  }
  
  return(p_value)
}

# Calcular p-valores
p_year <- calculate_LRT_p_value(beta_NFR_NFL, beta_NFR_NFL_year)
p_site <- calculate_LRT_p_value(beta_NFR_NFL, beta_NFR_NFL_site)
p_phenotype <- calculate_LRT_p_value(beta_NFR_NFL, beta_NFR_NFL_phenotype)

# Crear un data frame con los resultados
results_NFR_NFL <- data.frame(
  Component = c("Year", "Site", "Phenotype", "Residual"),
  r = c(r_year, r_site, r_phenotype, r_residual),
  p_value = c(p_year, p_site, p_phenotype, NA)  # No hay p-value para el residual en este contexto
)

# Mostrar la tabla
print(results_NFR_NFL)

```



## Peso seco fruto

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

graficos_list$PSF

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE, out.width='75%')

datosfisico_clean <- datosfisico[!is.na(datosfisico$PSF), ]
# datosfisico_clean$PSF_log <- log(datosfisico_clean$PSF)+3

# library(rptR)
# library(pbapply)
# rpt_PSF_log<-rptGaussian((PSF_log) ~ (1|year) + (1|site) + (1|phenotype),
#                     grname=c("year", "site", "phenotype", "Residual"),
#                     data=datosfisico_clean, nboot=3, npermut=3) #, ratio =FALSE
# summary(rpt_PSF_log)

# Mtest <- lmer((PSF_log) ~ (1|year) + (1|site) + (1|site/phenotype),
# data=datosfisico_clean)

```

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

library(lme4)

# Ajustar el modelo completo con distribución Gamma
gamma_PSF <- tryCatch({
  glmer(PSF ~ 1 + (1|year) + (1|site) + (1|phenotype), 
        data = frutos_fisico, 
        family = Gamma(link = "log"),
        control = glmerControl(optimizer = "bobyqa"))
}, error = function(e) {
  warning("Error ajustando el modelo completo: ", e$message)
  return(NULL)
})

if (is.null(gamma_PSF)) {
  stop("No se pudo ajustar el modelo completo para PSF.")
}

# Extraer varianzas
var_comps <- VarCorr(gamma_PSF)
var_year <- tryCatch(as.numeric(var_comps$year[1]), error = function(e) NA)
var_site <- tryCatch(as.numeric(var_comps$site[1]), error = function(e) NA)
var_phenotype <- tryCatch(as.numeric(var_comps$phenotype[1]), error = function(e) NA)
var_residual <- tryCatch(attr(var_comps, "sc")^2, error = function(e) NA)

total_var <- sum(var_year, var_site, var_phenotype, var_residual, na.rm = TRUE)
r_year <- (var_year / total_var) * 100
r_site <- (var_site / total_var) * 100
r_phenotype <- (var_phenotype / total_var) * 100
r_residual <- (var_residual / total_var) * 100

# Ajustar modelos reducidos
gamma_PSF_year <- tryCatch({
  glmer(PSF ~ 1 + (1|site) + (1|phenotype), 
        data = frutos_fisico, 
        family = Gamma(link = "log"),
        control = glmerControl(optimizer = "bobyqa"))
}, error = function(e) {
  warning("Error ajustando el modelo reducido para year: ", e$message)
  return(NULL)
})

gamma_PSF_site <- tryCatch({
  glmer(PSF ~ 1 + (1|year) + (1|phenotype), 
        data = frutos_fisico, 
        family = Gamma(link = "log"),
        control = glmerControl(optimizer = "bobyqa"))
}, error = function(e) {
  warning("Error ajustando el modelo reducido para site: ", e$message)
  return(NULL)
})

gamma_PSF_phenotype <- tryCatch({
  glmer(PSF ~ 1 + (1|year) + (1|site), 
        data = frutos_fisico, 
        family = Gamma(link = "log"),
        control = glmerControl(optimizer = "bobyqa"))
}, error = function(e) {
  warning("Error ajustando el modelo reducido para phenotype: ", e$message)
  return(NULL)
})

# Función para calcular p-valor mediante LRT, ajustada según rptR
calculate_LRT_p_value <- function(full_model, reduced_model) {
  if (is.null(full_model) || is.null(reduced_model)) return(NA)
  
  logLik_full <- logLik(full_model)
  logLik_reduced <- logLik(reduced_model)
  
  # Diferencia de log-verosimilitud multiplicada por 2
  LR_D <- -2 * (logLik_reduced - logLik_full)
  
  # Grados de libertad (esto puede variar dependiendo del número de términos aleatorios removidos)
  df <- 1  # Asumiendo que quitamos un solo término aleatorio
  
  # P-valor usando chi-cuadrado
  p_value <- pchisq(LR_D, df, lower.tail = FALSE)
  
  # Corrección para prueba unilateral cuando solo se elimina un parámetro
  if (df == 1 && LR_D > 0) {
    p_value <- p_value / 2
  }
  
  return(p_value)
}

# Calcular p-valores
p_year <- calculate_LRT_p_value(gamma_PSF, gamma_PSF_year)
p_site <- calculate_LRT_p_value(gamma_PSF, gamma_PSF_site)
p_phenotype <- calculate_LRT_p_value(gamma_PSF, gamma_PSF_phenotype)

# Crear un data frame con los resultados
results_PSF <- data.frame(
  Component = c("Year", "Site", "Phenotype", "Residual"),
  r = c(r_year, r_site, r_phenotype, r_residual),
  p_value = c(p_year, p_site, p_phenotype, NA)  # No hay p-value para el residual en este contexto
)

# Mostrar la tabla
print(results_PSF)

```

## Peso seco semillas

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

graficos_list$PSS

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE, out.width='75%')

datosfisico_clean <- datosfisico[!is.na(datosfisico$PSS), ]
datosfisico_clean$PSS_log <- log(datosfisico_clean$PSS)+5
datosfisico_clean <- datosfisico_clean[datosfisico_clean$PSS > 0, ]

library(rptR)
library(pbapply)
rpt_PSS_log<-rptGaussian((PSS_log) ~ (1|year) + (1|site) + (1|phenotype),
                    grname=c("year", "site", "phenotype", "Residual"),
                    data=datosfisico_clean, nboot=3, npermut=3)
summary(rpt_PSS_log)

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE, out.width='75%')

library(lme4)

# Ajustar el modelo completo con distribución Gamma
gamma_PSS <- tryCatch({
  glmer(PSS ~ 1 + (1|year) + (1|site) + (1|phenotype), 
        data = datosfisico_clean, 
        family = Gamma(link = "log"),
        control = glmerControl(optimizer = "bobyqa"))
}, error = function(e) {
  warning("Error ajustando el modelo completo: ", e$message)
  return(NULL)
})

if (is.null(gamma_PSS)) {
  stop("No se pudo ajustar el modelo completo para PSS.")
}

# Extraer varianzas
var_comps <- VarCorr(gamma_PSS)
var_year <- tryCatch(as.numeric(var_comps$year[1]), error = function(e) NA)
var_site <- tryCatch(as.numeric(var_comps$site[1]), error = function(e) NA)
var_phenotype <- tryCatch(as.numeric(var_comps$phenotype[1]), error = function(e) NA)
var_residual <- tryCatch(attr(var_comps, "sc")^2, error = function(e) NA)

total_var <- sum(var_year, var_site, var_phenotype, var_residual, na.rm = TRUE)
r_year <- (var_year / total_var) * 100
r_site <- (var_site / total_var) * 100
r_phenotype <- (var_phenotype / total_var) * 100
r_residual <- (var_residual / total_var) * 100

# Ajustar modelos reducidos
gamma_PSS_year <- tryCatch({
  glmer(PSS ~ 1 + (1|site) + (1|phenotype), 
        data = datosfisico_clean, 
        family = Gamma(link = "log"),
        control = glmerControl(optimizer = "bobyqa"))
}, error = function(e) {
  warning("Error ajustando el modelo reducido para year: ", e$message)
  return(NULL)
})

gamma_PSS_site <- tryCatch({
  glmer(PSS ~ 1 + (1|year) + (1|phenotype), 
        data = datosfisico_clean, 
        family = Gamma(link = "log"),
        control = glmerControl(optimizer = "bobyqa"))
}, error = function(e) {
  warning("Error ajustando el modelo reducido para site: ", e$message)
  return(NULL)
})

gamma_PSS_phenotype <- tryCatch({
  glmer(PSS ~ 1 + (1|year) + (1|site), 
        data = datosfisico_clean, 
        family = Gamma(link = "log"),
        control = glmerControl(optimizer = "bobyqa"))
}, error = function(e) {
  warning("Error ajustando el modelo reducido para phenotype: ", e$message)
  return(NULL)
})

# Función para calcular p-valor mediante LRT, ajustada según rptR
calculate_LRT_p_value <- function(full_model, reduced_model) {
  if (is.null(full_model) || is.null(reduced_model)) return(NA)
  
  logLik_full <- logLik(full_model)
  logLik_reduced <- logLik(reduced_model)
  
  # Diferencia de log-verosimilitud multiplicada por 2
  LR_D <- -2 * (logLik_reduced - logLik_full)
  
  # Grados de libertad (esto puede variar dependiendo del número de términos aleatorios removidos)
  df <- 1  # Asumiendo que quitamos un solo término aleatorio
  
  # P-valor usando chi-cuadrado
  p_value <- pchisq(LR_D, df, lower.tail = FALSE)
  
  # Corrección para prueba unilateral cuando solo se elimina un parámetro
  if (df == 1 && LR_D > 0) {
    p_value <- p_value / 2
  }
  
  return(p_value)
}

# Calcular p-valores
p_year <- calculate_LRT_p_value(gamma_PSS, gamma_PSS_year)
p_site <- calculate_LRT_p_value(gamma_PSS, gamma_PSS_site)
p_phenotype <- calculate_LRT_p_value(gamma_PSS, gamma_PSS_phenotype)

# Crear un data frame con los resultados
results_PSS <- data.frame(
  Component = c("Year", "Site", "Phenotype", "Residual"),
  r = c(r_year, r_site, r_phenotype, r_residual),
  p_value = c(p_year, p_site, p_phenotype, NA)  # No hay p-value para el residual en este contexto
)

# Mostrar la tabla
print(results_PSS)

```

## peso seco de semilla/peso seco de fruto

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

graficos_list$PSS_PSF

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE, out.width='75%')

datosfisico_clean <- datosfisico[!is.na(datosfisico$PSS_PSF), ]

library(rptR)
library(pbapply)
rpt_PSS_PSF<-rptGaussian((PSS_PSF) ~ (1|year) + (1|site) + (1|phenotype),
                    grname=c("year", "site", "phenotype", "Residual"),
                    data=datosfisico_clean, nboot=3, npermut=3)
summary(rpt_PSS_PSF)
# 
# Gaussian_PSS_PSF <- lmer(PSS_PSF ~ (1|year) + (1|site/phenotype),
#                data = datosfisico_clean,
#               )
# 
# residuals_Gaussian <- residuals(Gaussian_PSS_PSF)
# library(car)
# # Gráfico Q-Q para chequear normalidad
# qqPlot(residuals_Gaussian, main = "Q-Q Plot de Residuos")
# 
# # Prueba de Shapiro-Wilk para normalidad (aunque menos recomendada para grandes muestras)
# shapiro.test(residuals_Gaussian)
# 
# # Gráfico de residuos vs valores ajustados para chequear homogeneidad de varianzas
# plot(fitted(Gaussian_PSS_PSF), residuals_Gaussian,
#      xlab = "Valores Ajustados", ylab = "Residuos",
#      main = "Residuos vs Valores Ajustados")
# abline(h = 0, col = "red")

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE, out.width='75%')


library(lme4)
library(ggplot2)
library(dplyr)

# Ajuste del modelo con lmer (NO es BETA)
lmer_PSS_PSF <- lmer(PSS_PSF ~ 1 + (1|year) + (1|site) + (1|phenotype), data = datosfisico_clean)

# Extraer efectos aleatorios
random_effects <- ranef(lmer_PSS_PSF)

# Para 'year'
year_effects <- random_effects$year %>% 
                as.data.frame() %>% 
                dplyr::mutate(grp = rownames(.),
                       estimate = `(Intercept)`,
                       sd = as.data.frame(VarCorr(lmer_PSS_PSF))$sdcor[which(names(VarCorr(lmer_PSS_PSF)) == "year")])

ggplot(data = year_effects, aes(x = grp, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = estimate - sd, ymax = estimate + sd), width = 0.2) +
  labs(title = "Efectos Aleatorios por Año", x = "Año", y = "Efecto Aleatorio") +
  theme_minimal()

# Para 'site'
site_effects <- random_effects$site %>% 
                as.data.frame() %>% 
                dplyr::mutate(grp = rownames(.),
                       estimate = `(Intercept)`,
                       sd = as.data.frame(VarCorr(lmer_PSS_PSF))$sdcor[which(names(VarCorr(lmer_PSS_PSF)) == "site")])

ggplot(data = site_effects, aes(x = grp, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = estimate - sd, ymax = estimate + sd), width = 0.2) +
  labs(title = "Efectos Aleatorios por Sitio", x = "Sitio", y = "Efecto Aleatorio") +
  theme_minimal()

# Para 'phenotype'
phenotype_effects <- random_effects$phenotype %>% 
                     as.data.frame() %>% 
                     dplyr::mutate(grp = rownames(.),
                            estimate = `(Intercept)`,
                            sd = as.data.frame(VarCorr(lmer_PSS_PSF))$sdcor[which(names(VarCorr(lmer_PSS_PSF)) == "phenotype")])

phenotype_effects_ordenado <- phenotype_effects[order(phenotype_effects$estimate, decreasing = TRUE), ]

# Convertir 'grp' a factor, manteniendo el orden del data frame ordenado
phenotype_effects_ordenado$grp <- factor(phenotype_effects_ordenado$grp, levels = phenotype_effects_ordenado$grp)

# Ahora el gráfico reflejará el orden
ggplot(data = phenotype_effects_ordenado, aes(x = grp, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = estimate - sd, ymax = estimate + sd), width = 0.2) +
  labs(title = "Efectos Aleatorios por Fenotipo", x = "Fenotipo", y = "Efecto Aleatorio") +
  theme_minimal()

```


# Cálculo de (\( R^2 \)) y (\( \eta_0 \))

Cruz, C. D., Regazzi, A. J., & Carneiro, P. C. S. (2004). Modelos biométricos aplicados ao melhoramento genético. Viçosa: Universidade Federal de Viçosa.

Metodología para determinar el número óptimo de mediciones y el coeficiente de determinación (\( R^2 \))

Esta sección describe cómo determinar el número mínimo de mediciones necesarias (\( \eta_0 \)) para alcanzar un nivel deseado de precisión y cómo calcular el coeficiente de determinación (\( R^2 \)) para un número dado de mediciones.

## Fórmulas matemáticas

### Coeficiente de determinación (\( R^2 \))

El coeficiente de determinación mide qué tan bien el valor fenotípico promedio estimado a partir de \( n \) mediciones predice el valor verdadero del genotipo. Se calcula como:

\[
R^2 = \frac{n \cdot r}{1 + (n - 1) \cdot r}
\]

Donde:
- \( R^2 \): Coeficiente de determinación para \( n \) mediciones.
- \( n \): Número de mediciones realizadas.
- \( r \): Coeficiente de repetibilidad individual, que representa la proporción de la varianza total explicada por las diferencias permanentes entre individuos.

### Número mínimo de mediciones necesarias (\( \eta_0 \))

El número mínimo de mediciones necesarias para alcanzar un coeficiente de determinación deseado (\( R^2_{\text{target}} \)) se calcula como:

\[
\eta_0 = \frac{R^2_{\text{target}} \cdot (1 - r)}{(1 - R^2_{\text{target}}) \cdot r}
\]

Donde:
- \( \eta_0 \): Número mínimo de mediciones necesarias.
- \( R^2_{\text{target}} \): Nivel de precisión deseado (por ejemplo, 0.95 para un 95% de determinación).
- \( r \): Coeficiente de repetibilidad individual.

## Interpretación

1. **Coeficiente de determinación (\( R^2 \)):**
   - Si el valor de \( R^2 \) es bajo, las mediciones realizadas no predicen de manera confiable el valor verdadero del genotipo.
   - Un \( R^2 \) alto (por ejemplo, \( > 0.95 \)) indica una alta precisión en la estimación del valor fenotípico promedio.

2. **Número mínimo de mediciones (\( \eta_0 \)):**
   - Este cálculo indica cuántas mediciones repetidas se requieren para alcanzar el nivel de precisión deseado (\( R^2_{\text{target}} \)).
   - Si el coeficiente de repetibilidad (\( r \)) es bajo, \( \eta_0 \) será alto, lo que significa que se necesitarán más mediciones para compensar la baja precisión.
   
```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE, out.width='75%')

# # Parámetros
# r <- 0.257   # Coeficiente de repetibilidad individual
# R2_target <- 0.80  # Coeficiente de determinación deseado
# m <- 1:50  # Rango de mediciones (1 a 50)
# 
# # Fórmula para calcular el número mínimo de mediciones necesarias (η₀)
# eta0 <- (R2_target * (1 - r)) / ((1 - R2_target) * r)
# cat("Número mínimo de mediciones necesarias (η₀):", ceiling(eta0), "\n")
# 
# # Fórmula para calcular R² en función del número de mediciones (m)
# R2 <- function(m, r) {
#   (m * r) / (1 + (m - 1) * r)
# }
# 
# # Calcular R² para cada m
# R2_values <- sapply(m, R2, r = r)
# 
# # Resultados: mostrar R² por m
# data.frame(m = m, R2 = R2_values)


```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE, out.width='75%')

# Nombres de las variables
variables <- c(
  "PSFL", "DMAP", "DMIP", "NO", "NS", "NS/NO", "PSF", "PSS", "PSS/PSF", "NFR/NFL",
  "NFL/cm", "PFF", "DPF", "DMAF", "DMAF/DPF", "L", "a", "b", "IM", "PEN", "CLa",
  "CLb", "CTC", "CTC/CL", "ATT", "TA", "ATT/TA", "DPPH", "PT"
)

repetibilidad <- c(
  25.7, 2.4, 2.6, 12.5, 0.001, 2.3, 20.5, 20.6, 34.1, 0.001, 15.7, 
  19.4, 35.1, 25.8, 42.1, 16.9, 21.6, 21.3, 1.6, 14.7, 24.5, 24.4, 
  24.9, 27.5, 15.3, 32.7, 30.0, 2.2, 34.0
) / 100  # Convertir a proporción (porcentaje a decimal)

# Crear un data frame
df_repetibilidad <- data.frame(Variable = variables, Repetibilidad = repetibilidad)

# Valores de R2_target a calcular
R2_targets <- c(0.80, 0.90, 0.95, 0.99)

# Función para calcular η₀ (número mínimo de mediciones necesarias)
calculate_eta0 <- function(R2_target, r) {
  (R2_target * (1 - r)) / ((1 - R2_target) * r)
}

# Calcular η₀ para cada variable y R2_target
results <- do.call(rbind, lapply(1:nrow(df_repetibilidad), function(i) {
  r <- df_repetibilidad$Repetibilidad[i]
  eta0_values <- sapply(R2_targets, calculate_eta0, r = r)
  data.frame(
    Variable = df_repetibilidad$Variable[i],
    R2_80 = ceiling(eta0_values[1]),
    R2_90 = ceiling(eta0_values[2]),
    R2_95 = ceiling(eta0_values[3]),
    R2_99 = ceiling(eta0_values[4])
  )
}))

# Mostrar resultados
print(results)

```

# Análisis multivariado
```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# Tablas y gráficos resumen
library(dplyr)
# Crea un vector con las variables respuesta
# Crear un vector con los nombres específicos de las variables
variables_respuesta <- c("PSFL", "DMAP", "DMIP", "NO", "NS", "NS_NO")

# Inicializa listas para almacenar las tablas y gráficos
tablas_list_multivar <- list()

# Crea un bucle para recorrer cada variable respuesta
for (variable in variables_respuesta) {

  # Crea la tabla resumen
  assign(paste0("Tabla_", variable), datosff %>%
    dplyr::filter(!is.na(get(variable))) %>% # Filtra los datos sin NA para la variable actual
    dplyr::group_by(year, site, phenotype) %>%
    dplyr::summarise(!!sym(variable) := mean(!!sym(variable))
              )
  )
  
  # Agrega la tabla a la lista
  tablas_list_multivar[[variable]] <- get(paste0("Tabla_", variable))
  
}


```

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# Tablas resumen
library(dplyr)
# Crea un vector con las variables respuesta
# Crear un vector con los nombres específicos de las variables
variables_respuesta <- c("NFL_cm", "NFR_NFL")

# Crea un bucle para recorrer cada variable respuesta
for (variable in variables_respuesta) {

  # Crea la tabla resumen
  assign(paste0("Tabla_", variable), datosramas %>%
    dplyr::filter(!is.na(get(variable))) %>% # Filtra los datos sin NA para la variable actual
    dplyr::group_by(year, site, phenotype) %>%
    dplyr::summarise(!!sym(variable) := mean(!!sym(variable))
              )
  )
  
  # Agrega la tabla a la lista
  tablas_list_multivar[[variable]] <- get(paste0("Tabla_", variable))
  
}


```

```{r , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# Tablas resumen
library(dplyr)
# Crea un vector con las variables respuesta
# Crear un vector con los nombres específicos de las variables
variables_respuesta <- c("PSS", "PSF", "PSS_PSF")

# Crea un bucle para recorrer cada variable respuesta
for (variable in variables_respuesta) {

  # Crea la tabla resumen
  assign(paste0("Tabla_", variable), datosfisico %>%
    dplyr::filter(!is.na(get(variable))) %>% # Filtra los datos sin NA para la variable actual
    dplyr::group_by(year, site, phenotype) %>%
    dplyr::summarise(!!sym(variable) := mean(!!sym(variable))
              )
  )
  
  # Agrega la tabla a la lista
  tablas_list_multivar[[variable]] <- get(paste0("Tabla_", variable))
  
}

```

```{r , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)


library(purrr)
tabla_combinada <- reduce(tablas_list_multivar, full_join, by = c("year", "site", "phenotype"))

library(dplyr)
tabla_combinada_ordenada <- tabla_combinada %>%
  arrange(year, site, phenotype)

# Usando dplyr y stringr para crear la nueva columna
library(dplyr)
library(stringr)

# Crear la nueva columna 'year*site' concatenando los valores de 'year' y 'site'
tabla_combinada_ordenada <- tabla_combinada_ordenada %>%
  mutate(`year.site` = str_c(site, year, sep = "."))

tabla_combinada_ordenada$year.site <- as.factor(tabla_combinada_ordenada$year.site)

```

```{r , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

tabla_combinada2 <- tabla_combinada_ordenada[,-c(1:3,15)]

# Nombres de las variables
nombres_variables <- c("PSFL", "DMAP", "DMIP", "NO", "NS", "NS/NO", "NFL/cm", "NFR/NFL","PSS", "PSF", "PSS/PSF")
colnames(tabla_combinada2) <- nombres_variables

library(missMDA)
library(doParallel)
# Imputar valores faltantes usando PCA factorial
data_imputed <- imputePCA(tabla_combinada2,
                          ncp = 10)

tabla_pca <- (data_imputed$fittedX)

# Estandarizar las columnas numéricas en la tabla
tabla_pca_estandarizada <- scale(tabla_pca)

tabla_pca_estandarizada <- as.data.frame(tabla_pca_estandarizada)

# Nombres de las variables
colnames(tabla_pca_estandarizada) <- nombres_variables

tabla_pca_estandarizada$year <- tabla_combinada_ordenada$year
tabla_pca_estandarizada$site <- tabla_combinada_ordenada$site
tabla_pca_estandarizada$phenotype <- tabla_combinada_ordenada$phenotype
tabla_pca_estandarizada$year.site <- tabla_combinada_ordenada$year.site

tabla_cca_means <- tabla_pca_estandarizada

```

# Análisis de PCA

```{r, echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

#Load library
library(readxl)

#Create object "datamet" which contains climate data
datamet <- read_excel("database.xlsx", sheet = "II_ER_SMN")
#Delete unnecessary columns
datamet<-datamet[,-c(11:28)]
#
datamet$site<-as.factor(datamet$site)
datamet$year<-as.factor(datamet$year)
datamet$date_long<-as.POSIXct(datamet$date_long,format="%m-%d-%Y")
datamet$site<-factor(datamet$site,levels=c("concordia", "palmar","gualeguaychu"))
datamet$site<-factor(datamet$site,labels=c("Concordia", "PN El Palmar","Gualeguaychú"))

library(dplyr)
library(lubridate)

# Filtrar las filas desde el 1 de junio (mes 6, día 1) al 1 de noviembre (mes 11, día 1)
datamet_filtered <- datamet %>%
  filter((month(date_long) == 5 & day(date_long) >= 1) |  
         (month(date_long) == 6) |
         (month(date_long) == 7) |
         (month(date_long) == 8) |
         (month(date_long) == 9) |
         (month(date_long) == 10) |
         (month(date_long) == 11 & day(date_long) <= 1)) 

var_mets <- c("temp", "hr", "wind", "prec")

datamet_means <- datamet_filtered %>%
  dplyr::group_by(year, site) %>%
  dplyr::summarise(across(all_of(var_mets), mean, na.rm = TRUE))

colnames(datamet_means)[colnames(datamet_means) == "wind"] <- "vv"
colnames(datamet_means)[colnames(datamet_means) == "prec"] <- "pp"


```

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# Asegurar que las variables estén correctamente escaladas para el PCA
# Desagrupar la tabla antes de escalar
variables_respuesta <- tabla_cca_means %>%
  ungroup() %>%  # Eliminar el agrupamiento
  dplyr::select(PSFL, DMAP, DMIP, NO, NS, NS_NO = `NS/NO`, NFL_cm = `NFL/cm`, NFR_NFL = `NFR/NFL`, PSF, PSS, `PSS/PSF`) %>%
  scale()  # Escalar las variables numéricas

# Calcular las medias de las variables suplementarias agrupadas por year y site
variables_suplementarias <- datamet_means %>%
  dplyr::group_by(year, site) %>%
  dplyr::summarise(across(c(temp, pp, hr, vv), mean, na.rm = TRUE)) 

# Unir las variables climáticas promedio a la tabla de respuestas
tabla_merged0 <- tabla_cca_means %>%
  left_join(variables_suplementarias, by = c("year", "site"))


```

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

library(FactoMineR)
library(factoextra)

tabla_merged <- tabla_merged0[,-c(16:19)]

pca_result <- PCA(tabla_merged,
                   quali.sup = c(12:15),  # year, site, phenotype
                   # quanti.sup = c(16:19),  # Las variables climáticas suplementarias
                  graph = FALSE)
```

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# # Visualización de los individuos (fenotipos)
# fviz_pca_ind(pca_result, 
#              # geom.ind = "point", 
#              col.ind = tabla_cca_means$phenotype, # Colorear por fenotipo
#              addEllipses = FALSE, ellipse.level = 0.95,
#              pointsize = 4,
#              label = "none") +
#   labs(title = "PCA - Fenotipos")+
#     theme(legend.position = 'none')+
#   coord_cartesian(xlim = c(-7, 7), ylim = c(-6, 6))
# 
# # Visualización de las variables activas y suplementarias
# fviz_pca_var(pca_result, 
#              col.var = "black", 
#              repel = TRUE) + 
#   labs(title = "PCA - Variables Activas y Suplementarias")


```

Resultado PCA
```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

pca_scores <- as.data.frame(pca_result$ind$coord)
pca_scores$year <- tabla_cca_means$year
pca_scores$site <- tabla_cca_means$site
pca_scores$phenotype <- tabla_cca_means$phenotype
pca_scores$year.site <- tabla_cca_means$year.site

# Ver resumen del PCA
summary(pca_result)

```

Scree plot para ver la varianza explicada por cada Principal component
```{r , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=24, fig.height=16}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

library(factoextra)
library(ggplot2)

# # Calcular los valores propios del PCA
# eig_values <- pca_result$eig[, 2]  # Extraer la primera columna de eigenvalues

# # Crear el gráfico de barras con etiquetas ajustadas
# gg_scree_repro <- fviz_eig(pca_result, addlabels = FALSE) +
#   geom_bar(stat = "identity", aes(y = eig_values, x = factor(1:length(eig_values))), fill = "#FB9B99") +
#   geom_text(aes(label = round(eig_values, 2), y = eig_values, x = factor(1:length(eig_values)), vjust = -0.5), size = 12) +
#   theme_classic() +
#   labs(title = "Scree Plot",
#        x = "Componente Principal",
#        y = "Porcentaje de Varianza [%]") +
#   theme(text = element_text(size = 32, color = 'black'),
#         axis.text = element_text(size = 32, color = 'black'),
#         legend.position = "none") +
#   scale_y_continuous(limits = c(0, max(eig_values) * 1.1))+
#   geom_text(aes(x = Inf, y = Inf, label = "A"), size = 16)
# 
# # Mostrar el gráfico
# print(gg_scree_repro)


```

Contribuciones de las variables para CP1
```{r , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=24, fig.height=16}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# Obtener las contribuciones de las variables a cada Principal component
pca_var <- get_pca_var(pca_result)

```

Contribuciones de las variables para CP2
```{r , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# Ordenar las variables por contribución para Dim.1 (PC1)
pca_var_dim1 <- pca_var$contrib[, 1] %>% sort(decreasing = TRUE)
pca_var_dim1

# Ordenar las variables por contribución para Dim.2 (PC2)
pca_var_dim2 <- pca_var$contrib[, 2] %>% sort(decreasing = TRUE)
pca_var_dim2

```

Contribución acumulada de cada variable a CP1 y CP2
```{r, echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# Asegurarse de que los nombres de las variables coincidan entre los dos vectores
common_names <- intersect(names(pca_var_dim1), names(pca_var_dim2))

# Filtrar solo las variables comunes
pca_var_dim1_filtered <- pca_var_dim1[common_names]
pca_var_dim2_filtered <- pca_var_dim2[common_names]
# Sumar las contribuciones de cada variable en los dos componentes
combined_contributions <- pca_var_dim1_filtered + pca_var_dim2_filtered

# Ordenar las variables por su contribución acumulada
sorted_contributions <- sort(combined_contributions, decreasing = TRUE)

# Mostrar el resultado
sorted_contributions

```

Proporción de contribución de cada variable a PC1, PC2 y PC3
```{r , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=24, fig.height=16}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)
# Suponiendo que tienes el objeto pca_result con la salida de tu análisis PCA
library(ggplot2)
library(dplyr)
library(tidyr)

# Extraer las rotaciones (cargas) de los componentes principales
loadings <- as.data.frame(pca_result$var$coord)

# Seleccionar solo las cargas para PC1 y PC2
loadings <- loadings %>%
  dplyr::select(Dim.1, Dim.2) #, Dim.3

# Calcular la proporción de contribución para cada parámetro en cada componente
loadings <- loadings %>%
  dplyr::mutate(Variable = rownames(loadings),
         PC1 = abs(Dim.1) / sum(abs(Dim.1)),
         PC2 = abs(Dim.2) / sum(abs(Dim.2))#,
         # PC3 = abs(Dim.3) / sum(abs(Dim.3))
         )

colnames(loadings)[colnames(loadings) == "PC1"] <- "CP1"
colnames(loadings)[colnames(loadings) == "PC2"] <- "CP2"

# Convertir los datos a formato largo (long format)
loadings_long <- loadings %>%
  pivot_longer(cols = starts_with("CP"),
               names_to = "Component",
               values_to = "Contribution")

library(RColorBrewer)
# "Set1", "Set2", "Set3", "Pastel1", "Pastel2", "Paired", "Dark2", "Accent"

# Crear el gráfico de barras apiladas
gg_bar_agr <- ggplot(loadings_long, aes(x = Component, y = Contribution * 100, fill = Variable)) +
  geom_bar(stat = "identity", size = 5) +
  geom_text(aes(label = paste0(round(Contribution * 100, 1), "%")), 
            position = position_stack(vjust = 0.5), 
            size = 6, color = "black") +
  scale_fill_brewer(palette = "Paired") +
  labs(title = "",
       x = "Componente principal",
       y = "Contribución de cada variable") +
  theme_classic() +
  theme(text = element_text(size = 32, color = 'black'),
        legend.position = "right",
        legend.title = element_blank()) +
  theme(axis.text = element_text(color = 'black')) +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) +
  scale_x_discrete(labels = c(1,2)) +
  geom_text(aes(x = 3.5, y = 95, label = ""), size = 16, hjust = 0.5, vjust = 0.5) 

gg_bar_agr
# ggsave("gg_bar_agr", plot = gg_bar_agr, device = "tiff", width = 16, height = 8, units = "in", dpi = 300)

```

## Biplots de PCA CP1 VS CP2
```{r , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=24, fig.height=16}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# c("#A6CEE3", "#1F78B4", "#B2DF8A", "#33A02C", "#FB9A99",
# "#E31A1C", "#FDBF6F", "#FF7F00", "#CAB2D6", "#6A3D9A",
# "#FFFF00", "#B15928")

# Cambiar los nombres de las variables suplementarias en el objeto PCA
# rownames(pca_result$quanti.sup$coord) <- c("T", "PP", "HR", "V")

library(factoextra)
library(ggplot2)

# Crear el biplot
biplot_sites_repro <- fviz_pca_biplot(pca_result, 
                                       axes = c(1, 2), # Especifica CP1 y CP2
                                       geom.ind = "point", 
                                       label = "var", # Etiquetas solo para las variables
                                       addEllipses = FALSE,
                                       labelsize = 6, 
                                       col.var = "black", 
                                       repel = TRUE) +
  # Añadir los puntos de los fenotipos
  geom_point(data = pca_scores, 
             aes(x = Dim.1, y = Dim.2, color = site), size = 5) + 
  scale_color_manual(values = c("coral1", "burlywood2", "palegreen4"),
                     name = NULL) +
  theme_classic() +
  labs(title = "", x = "PC1 (30,2 %)", y = "PC2 (18,0 %)") +
  theme(legend.position = "bottom",
        text = element_text(size = 26, color = 'black'),
        axis.text = element_text(color = 'black')) +
  geom_text(aes(x = Inf, y = Inf, label = ""), 
            hjust = 1.2, vjust = 1.2, size = 16, color = "black", fontface = "bold")+
  coord_cartesian(xlim = c(-5, 5), ylim = c(-5, 5))
  
# Mostrar el gráfico
# print(biplot_sites_repro)

```

```{r , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=24, fig.height=16}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# c("#A6CEE3", "#1F78B4", "#B2DF8A", "#33A02C", "#FB9A99",
# "#E31A1C", "#FDBF6F", "#FF7F00", "#CAB2D6", "#6A3D9A",
# "#FFFF00", "#B15928")

biplot_years_repro <- fviz_pca_biplot(pca_result, 
                                       axes = c(1, 2), # Especifica CP1 y CP2
                                       geom.ind = "point", 
                                       label = "var", # Etiquetas solo para las variables
                                       addEllipses = FALSE,
                                       labelsize = 6, 
                                       col.var = "black", 
                                       repel = TRUE) +
  # Añadir los puntos de los fenotipos
  geom_point(data = pca_scores, 
             aes(x = Dim.1, y = Dim.2, color = year), size = 5) + 
  scale_color_manual(values = c("2019" = "#f7db20", "2021" = "#07c22c", "2022" = "#522DAD"),
                     name = NULL) +
  theme_classic() +
  labs(title = "", x = "PC1 (30,2 %)", y = "PC2 (18,0 %)") +
  theme(legend.position = "bottom",
        text = element_text(size = 26, color = 'black'),
        axis.text = element_text(color = 'black')) +
  geom_text(aes(x = Inf, y = Inf, label = ""), 
            hjust = 1.2, vjust = 1.2, size = 13, color = "black", fontface = "bold")+
  coord_cartesian(xlim = c(-5, 5), ylim = c(-5, 5))

# biplot_years_repro

```

```{r , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=24, fig.height=16}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

library(dplyr)
library(ggplot2)
library(factoextra)

# Suponiendo que pca_scores tiene las columnas Dim.1, Dim.2, y year.site
# Crear un dataframe para los contornos (convex hulls)
# hulls <- pca_scores %>%
#   group_by(phenotype) %>%
#   slice(chull(Dim.1, Dim.2)) # Calcula el contorno convexo

# Crear el gráfico PCA
biplot_year.site_repro <- fviz_pca_biplot(pca_result, 
                                           axes = c(1, 2), # Especifica CP1 y CP2
                                           geom.ind = "point", 
                                           label = "var", # Etiquetas solo para las variables
                                           addEllipses = FALSE,
                                           labelsize = 6, 
                                           col.var = "black", 
                                           repel = TRUE) +
  # Añadir los puntos de los fenotipos
  geom_point(data = pca_scores, 
             aes(x = Dim.1, y = Dim.2, color = year.site), size = 5) + 
  # Añadir los polígonos que representan el perímetro de los grupos
  # geom_polygon(data = hulls, 
  #              aes(x = Dim.1, y = Dim.2, fill = year.site), 
  #              alpha = 0.2, color = "black") +  # Color del perímetro
  scale_color_manual(values = c("coral1", "red", "orange", "palegreen4", "green","yellowgreen","burlywood2","burlywood4","yellow"),
                     name = NULL) +
  theme_classic() +
  labs(title = "", x = "PC1 (30,2 %)", y = "PC2 (18,0 %)") +
  theme(legend.position = "bottom",
        text = element_text(size = 26, color = 'black'),
        axis.text = element_text(color = 'black')) +
  geom_text(aes(x = Inf, y = Inf, label = ""), 
            hjust = 1.2, vjust = 1.2, size = 13, color = "black", fontface = "bold")+
  coord_cartesian(xlim = c(-5, 5), ylim = c(-5, 5))

# Mostrar el gráfico
print(biplot_year.site_repro)

```

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=24, fig.height=20}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# Cargar el paquete ggrepel para evitar la superposición de las etiquetas
library(ggrepel)
library(ggplot2)
library(factoextra)

# Crear el gráfico de PCA
gg_pca_repro <- fviz_pca_biplot(pca_result, 
                                geom.ind = "point",           # Puntos para los individuos
                                pointshape = 21,              # Forma de los puntos
                                pointsize = 6,                # Tamaño de los puntos
                                geom.var = c("point", "text"), # Puntos y texto para las variables
                                labelsize = 8,               # Tamaño del texto
                                fill.ind = tabla_merged$year, # Colorear los individuos por año
                                col.ind = "black",            # Color del borde de los puntos
                                repel = TRUE,                 # Evitar superposición de etiquetas
                                addEllipses = FALSE,          # No agregar elipses
                                label = "var",                # Etiquetar las variables
                                col.var = "black",            # Color de las variables basado en cos2
                                arrows = TRUE) + 

  # Colorear los puntos por year
  scale_fill_manual(values = c("2019" = "#f7db20", "2021" = "#07c22c", "2022" = "#522DAD"),
                    name = "Año") +
  
  # Añadir título y etiquetas de ejes
  labs(title = "", 
       x = "Componente Principal 1 (30,2 %)", 
       y = "Componente Principal 2 (18,0 %)") +
  
  # Personalizar el tema del gráfico
  theme_classic() +
  theme(legend.position = "bottom",
        text = element_text(size = 24, color = 'black'),
        axis.text = element_text(color = 'black')) +
  
  # Limitar los ejes para que los vectores sean visibles
  coord_cartesian(xlim = c(-5, 5), ylim = c(-4, 4))

# Muestra el gráfico
print(gg_pca_repro)

# Guardar el gráfico como PNG con alta resolución
# ggsave("gg_pca_repro.tiff", plot = gg_pca_repro, device = "tiff", width = 15, height = 10, units = "in", dpi = 300)

```

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=24, fig.height=16}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# Cargar el paquete ggrepel para evitar la superposición de las etiquetas
library(ggrepel)
library(ggplot2)
library(factoextra)

# Crear el gráfico de PCA
mi_grafico_pca_sites <- fviz_pca_biplot(pca_result, 
                                geom.ind = "point",           # Puntos para los individuos
                                pointshape = 21,              # Forma de los puntos
                                pointsize = 6,                # Tamaño de los puntos
                                geom.var = c("point", "text"), # Puntos y texto para las variables
                                labelsize = 8,               # Tamaño del texto
                                fill.ind = tabla_merged$site, # Colorear los individuos por año
                                col.ind = "black",            # Color del borde de los puntos
                                repel = TRUE,                 # Evitar superposición de etiquetas
                                addEllipses = FALSE,          # No agregar elipses
                                label = "var",                # Etiquetar las variables
                                col.var = "black", # Color de las variables basado en cos2
                                arrows = TRUE) + 
  # Colorear los puntos por year
  scale_fill_manual(values = c("coral1", "burlywood2", "palegreen4"),
                    name = NULL) +
  
  # Añadir título y etiquetas de ejes
  labs(title = "", 
       x = "Componente Principal 1 (30,2 %)", 
       y = "Componente Principal 2 (18,0 %)") +
  
  # Personalizar el tema del gráfico
  theme_classic() +
  theme(legend.position = "bottom",
        text = element_text(size = 24, color = 'black'),
        axis.text = element_text(color = 'black')) +
  # Añadir un título o marcador en el gráfico
  geom_text(aes(x = Inf, y = Inf, label = ""), 
            hjust = 1.2, vjust = 1.2, size = 12, color = "black", fontface = "bold") +
  # Limitar los ejes para que los vectores sean visibles
  coord_cartesian(xlim = c(-5, 5), ylim = c(-4, 4))

# Mostrar el gráfico
print(mi_grafico_pca_sites)

# Guardar el gráfico como PNG con alta resolución
# ggsave("mi_grafico_pca_sites.tiff", plot = mi_grafico_pca_sites, device = "tiff", width = 15, height = 10, units = "in", dpi = 300)

```

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# Para renderizar a pdf
# rmarkdown::render("ER_flores.Rmd", output_format = "pdf_document")

# tiff("gg_pca_repro.tiff", units="cm", width=16, height=10, res=300)
# dev.off()
```

# Heatmap
## Correlaciones

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE, out.width='75%')

library(Hmisc)
library(ggcorrplot)
library(corrplot)

datos <- tabla_combinada2
datos <- na.omit(datos)

# Calculamos la correlación de Spearman y los p-valores
resultado_corr <- rcorr(as.matrix(datos), type = "spearman")

# Calcula la matriz de correlación
matriz_corr <- cor(datos, use = "pairwise.complete.obs") # Asegúrate de que 'datos' tiene el formato correcto
p_valores <- cor.mtest(datos, conf.level = 0.95)$p  # Esto también debe coincidir con 'datos'

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=24, fig.height=24}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE, out.width='95%')

# Paquetes necesarios
# library(corrplot)

# Matriz de correlación (matriz_corr) y matriz de p-valores (p_valores) ya creadas

# Graficar la matriz de correlación, destacando los valores significativos con asteriscos
# heatmap <- corrplot(matriz_corr, 
#          method = "ellipse",      # Estilo de las formas, como en el gráfico
#          type = "lower",          # Mostrar solo la mitad inferior de la matriz
#          p.mat = p_valores,       # Matriz de p-valores para mostrar significancia
#          sig.level = c(0.001, 0.01, 0.05),  # Niveles de significancia
#          insig = "label_sig",     # Mostrar los asteriscos en los valores significativos
#          pch.cex = 1.5,           # Tamaño de los asteriscos
#          pch.col = "red",         # Color de los asteriscos
#          addCoef.col = "black",   # Añadir valores numéricos (correlaciones)
#          diag = FALSE,            # No mostrar la diagonal con las correlaciones 1.00
#          tl.col = "black",        # Color de las etiquetas
#          tl.srt = 45,             # Rotar las etiquetas para mejorar la legibilidad
#          tl.pos = "b",            # Posicionar etiquetas en la diagonal
#          tl.cex = 1.1,            # Tamaño de las etiquetas
#          tl.font = 2,             # Negrita en las etiquetas
#          number.cex = 0.9,        # Tamaño de los valores numéricos de correlación
#          number.font = 1,         # Fuente normal para los valores de correlación
#          cl.cex = 1.0,            # Tamaño de la barra de colores
#          cl.pos = "b",            # Mostrar la barra de colores en la parte inferior
#          mar = c(0, 0, 0, 0))     # Márgenes ajustados para que no se superpongan

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=10}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE, out.width='100%')

corrplot(matriz_corr, 
          method = "ellipse",     # Mostrar elipses en la parte superior
          type = "upper",         # Solo parte superior
          addCoef.col = "black",  # Añadir valores numéricos de correlación
          tl.pos = "d",           # Ocultar etiquetas en márgenes
          order = "AOE",          # No mostrar la diagonal de 1.0
          tl.col = "black")           


```

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=10}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# # Abre un dispositivo TIFF
# tiff("corrplot.tiff", units = "cm", width = 32, height = 24, res = 300)
# 
# # Genera el gráfico
# corrplot(matriz_corr, 
#          method = "ellipse",     # Mostrar elipses en la parte superior
#          type = "upper",         # Solo parte superior
#          addCoef.col = "black",  # Añadir valores numéricos de correlación
#          tl.pos = "d",           # Ocultar etiquetas en márgenes
#          order = "AOE",          # No mostrar la diagonal de 1.0
#          tl.col = "black")       
# 
# # Cierra el dispositivo gráfico
# dev.off()

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE, out.width='75%')

# library(readxl)
# ledesma <- read_excel("database.xlsx", sheet="V ledesma")
# 
# ledesma$year<-as.factor(ledesma$year)
# ledesma$site<-factor(ledesma$site, levels = c("Concordia", "PN El Palmar", "Gualeguaychú"))
# ledesma$phenotype<-as.factor(ledesma$phenotype)
# 
# library(dplyr)
# tabla_phenotypes <- datosff %>%
#   dplyr::group_by(year, site, phenotype) %>%
#   dplyr::summarise(PSFL_mean = mean(PSFL),
#             NO_mean = mean(NO),
#             mean_polen_mean = mean(mean_polen),
#             mean_ns = mean(NS))
# 
# df_completo <- merge(x=ledesma, y=tabla_phenotypes, by=c("year","site","phenotype"), all.y = TRUE, no.dups = TRUE)

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=25, fig.height=20}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# VRff <- df_completo[,-c(1:3)]
# 
# library(GGally)
# VRff_gg<-ggpairs(VRff)
# VRff_gg

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

### Correlación N° de semillas y N° de óvulos ---------------

# correlación número de óvulo y número de semilla

# cor_value_ovusem <- cor(df_completo$mean_ns, df_completo$NO_mean, use = "complete.obs")
# 
# library(ggplot2)
# cor_ovusem<-ggplot(df_completo, aes(x=NO_mean,y=mean_ns,color=site,shape=year))+
#   geom_point(size=6)+
#   labs(x="", y="")+
#   # facet_grid(year~.)+
#   theme_bw()+
#   theme(plot.title = element_text(hjust = 0.5, size = rel(1.5))) +
#   scale_color_manual(values =    c("coral1","burlywood2","palegreen4"))+
#   theme(legend.title=element_blank())+
#   theme(legend.position='bottom')+
#   theme(panel.grid.major.y = element_line(color='black'))+
#   theme(panel.grid.major.x = element_blank())+
#   theme(text = element_text(size=15, color='black'))+
#   theme(axis.text = element_text(color='black'))
# 
# cor_ovusem
# cor_value_ovusem

```

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# (VEA1 / VEA2) -> distinta pendiente (slope)
# (VEA1 | VEA2) -> distinta ordenada al origen (intercept)
# (VEA1 | VEA2) + (1|VR2) -> distinta pendiente y ordenada al origen

# df_ovusem <- df_completo[,c(1:3,13,15)]
# df_ovusem <- na.omit(df_ovusem)
# 
# # datosff_ovusem <- datosff[,c(1:4,6,13)]
# # datosff_ovusem <- na.omit(datosff_ovusem)
# 
# cor_ovusem2<-ggplot(df_ovusem, aes(x=NO_mean,y=mean_ns,color=site,shape=year))+
#   geom_point(size=6)+
#   labs(x="N° óvulos", y="N° semillas")+
#   facet_grid(year~.)+
#   theme_bw()+
#   theme(plot.title = element_text(hjust = 0.5, size = rel(1.5))) +
#   scale_color_manual(values =    c("coral1","burlywood2","palegreen4"))+
#   theme(legend.title=element_blank())+
#   theme(legend.position='bottom')+
#   theme(panel.grid.major.y = element_line(color='black'))+
#   theme(panel.grid.major.x = element_blank())+
#   theme(text = element_text(size=15, color='black'))+
#   theme(axis.text = element_text(color='black'))+
#   scale_x_continuous(breaks = 0:12, labels = c(0:12))
# 
# # cor_ovusem_phen <- ggplot(data = datosff_ovusem, aes(x = NO, y = ns, color = year)) +
# #   geom_point() +
# #   theme_bw() +
# #   facet_wrap(~ site) + labs(y = "N° semillas") +
# #   scale_color_manual(values =    c("maroon4","dodgerblue4","deeppink"))+
# #   theme(legend.position = "bottom")
# 
# library(lme4)
# library(lmerTest)
# 
# # fit_ovusem4 <- lm(mean_ns ~ NO_mean, data = df_ovusem)
# 
# # fit_ovusem3 <- lmer(ns ~ NO + (1 |year) * (1 | site/phenotype), data = datosff)
# 
# # fit_ovusem2 <- lmer(mean_ns ~ NO_mean + (1 |year) + (1 | site), data = df_ovusem)
# 
# fit_ovusem <- lmer(log(mean_ns) ~ NO_mean + (1 |year) + (1 | site), data = df_ovusem)
# fit_ovusem
# # null_model <- lmer(mean_ns ~ (1 | year) + (1 | site), data = df_ovusem)
# # anova(fit_ovusem, null_model)
# 
# #summary(fit_ovusem)

```

```{r, echo=FALSE, error=TRUE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# ovusem_ajuste <- as.data.frame(cbind(
#   "residuos" = residuals(fit_ovusem),
#   "predichos" = predict(fit_ovusem)))
# 
# HV_ovusem<-ggplot(ovusem_ajuste) +
#   aes(predichos, residuos) +
#   geom_hline(yintercept = 0, colour="white", size=2) +
#   geom_point(colour="white")+
#   theme_classic()+
#   labs(y="Residuals", x="Predicted values")+
#   theme(legend.position = "bottom", legend.background = element_rect(fill = "gray12"))+
#   theme(panel.grid.major.y = element_line(size=0.05, color='white'))+
#   theme(panel.grid.major.x = element_blank())+
#   theme(text = element_text(size=10, color='white'))+
#   theme(axis.text = element_text(color='white'))+
#   theme(plot.background = element_rect(fill = "gray12"))+
#   theme(panel.background = element_rect(fill = "gray12", color = "white"))
# HV_ovusem
# 
# QQ_ovusem<-ggplot(ovusem_ajuste) +
#   aes(sample = residuos) +
#   geom_qq(shape = 1, colour="white") +
#   geom_qq_line(colour="white")+
#   theme_classic()+
#   labs(y="Sample Quantiles", x="Theoretical Quantiles")+
#   theme(legend.position = "bottom", legend.background = element_rect(fill = "gray12"))+
#   theme(panel.grid.major.y = element_line(size=0.05, color='white'))+
#   theme(panel.grid.major.x = element_blank())+
#   theme(text = element_text(size=10, color='white'))+
#   theme(axis.text = element_text(color='white'))+
#   theme(plot.background = element_rect(fill = "gray12"))+
#   theme(panel.background = element_rect(fill = "gray12", color = "white"))
# QQ_ovusem
# 
# e<-resid(fit_ovusem) # residuos de pearson
# # r.est <- resid(modelo_PSFlor, type="normalized") #residuos estandarizados
# pre<-predict(fit_ovusem) #predichos
# # alfai<-ranef(modelo_PSFlor)$$'(Intercept)'
# shapiro.test(e)

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

# library(dplyr)
# 
# tabla_antpol <- df_completo %>%
#   dplyr::group_by(site, year, phenotype) %>%
#   dplyr::summarise(n=n(),
#             Mean=mean(antesis_polinacion),
#             sd=sd(antesis_polinacion),
#             min=min(antesis_polinacion),
#             max=max(antesis_polinacion))
# 
# library(ggplot2)
# gg_ledesma<-ggplot(tabla_antpol, aes(x=site,y=Mean,fill=site))+
#   stat_summary(fun = "mean", size = 0.5, geom = "bar",position="dodge", width=0.2) +
#   # geom_errorbar(aes(min=min, max=max),width=.2, position=position_dodge(.5))+
#   labs(x="", y="%")+
#   facet_grid(year~.)+
#   theme_classic()+
#   theme(plot.title = element_text(hjust = 0.5, size = rel(1.5))) +
#   scale_fill_manual(values =    c("coral1","burlywood2","palegreen4"))+
#   theme(legend.title=element_blank())+
#   theme(legend.position='bottom')+
#   theme(panel.grid.major.y = element_line(color='black'))+
#   theme(panel.grid.major.x = element_blank())+
#   theme(text = element_text(size=15, color='black'))+
#   theme(axis.text = element_text(color='black'))
# # gg_ledesma

```

```{r  , echo=FALSE, fig.align='center', fig.asp=0.4, fig.width=10, fig.height=8}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE, out.width='75%')

# # Cálculo del número de muestras necesarias
# 
# # Parámetros
# r <- 0.456   # Coeficiente de repetibilidad población/individuo
# target_rm <- 0.8  # Precisión deseada
# m <- 5:30  # Rango de evaluaciones posibles
# 
# # Función para calcular r_m
# r_m <- function(m, r) {
#   (m * r) / (1 + (m - 1) * r)
# }
# 
# # Calcular r_m para cada m
# rm_values <- r_m(m, r)
# 
# # Mostrar resultados
# data.frame(m = m, r_m = rm_values)
# 
# # Identificar el mínimo m necesario
# min_m <- min(m[rm_values >= target_rm])
# cat("Número mínimo de evaluaciones necesarias:", min_m, "\n")


```
